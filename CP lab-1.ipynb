{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install numba","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T10:21:12.511447Z","iopub.execute_input":"2026-01-29T10:21:12.511783Z","iopub.status.idle":"2026-01-29T10:21:16.568036Z","shell.execute_reply.started":"2026-01-29T10:21:12.511746Z","shell.execute_reply":"2026-01-29T10:21:16.566963Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (0.60.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba) (0.43.0)\nRequirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.12/dist-packages (from numba) (2.0.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nfrom numba import cuda","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T10:22:10.902887Z","iopub.execute_input":"2026-01-29T10:22:10.903233Z","iopub.status.idle":"2026-01-29T10:22:12.741113Z","shell.execute_reply.started":"2026-01-29T10:22:10.903196Z","shell.execute_reply":"2026-01-29T10:22:12.740443Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"@cuda.jit\ndef hello_kernel():\n    # Thread and block indices\n    tx = cuda.threadIdx.x\n    bx = cuda.blockIdx.x\n    bdim = cuda.blockDim.x\n\n    # Global thread ID\n    gid = bx * bdim + tx\n\n    print(\"Hello from Block\", bx, \"Thread\", tx, \"Global ID\", gid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T10:24:03.717974Z","iopub.execute_input":"2026-01-29T10:24:03.718296Z","iopub.status.idle":"2026-01-29T10:24:03.723322Z","shell.execute_reply.started":"2026-01-29T10:24:03.718268Z","shell.execute_reply":"2026-01-29T10:24:03.722643Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"blocks = 2\nthreads_per_block = 4\n\nhello_kernel[blocks, threads_per_block]()\ncuda.synchronize()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T10:24:25.256327Z","iopub.execute_input":"2026-01-29T10:24:25.256674Z","iopub.status.idle":"2026-01-29T10:24:26.518490Z","shell.execute_reply.started":"2026-01-29T10:24:25.256649Z","shell.execute_reply":"2026-01-29T10:24:26.517127Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:680: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n  warn(NumbaPerformanceWarning(msg))\n","output_type":"stream"},{"name":"stdout","text":"Hello from Block 1 Thread 0 Global ID 4\nHello from Block 1 Thread 1 Global ID 5\nHello from Block 1 Thread 2 Global ID 6\nHello from Block 1 Thread 3 Global ID 7\nHello from Block 0 Thread 0 Global ID 0\nHello from Block 0 Thread 1 Global ID 1\nHello from Block 0 Thread 2 Global ID 2\nHello from Block 0 Thread 3 Global ID 3\n","output_type":"stream"}],"execution_count":14}]}